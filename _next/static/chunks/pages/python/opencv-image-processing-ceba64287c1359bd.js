(self.webpackChunk_N_E=self.webpackChunk_N_E||[]).push([[530],{76:(e,n,i)=>{(window.__NEXT_P=window.__NEXT_P||[]).push(["/python/opencv-image-processing",function(){return i(7635)}])},7635:(e,n,i)=>{"use strict";i.r(n),i.d(n,{default:()=>p});var s=i(7876),t=i(3923),a=i(8144),r=i(6004),o=i(3067),l=i(1621),h=i(9254),d=i(6815);let c=function(e){let{children:n}=e;return(0,s.jsx)(a.S,{children:n})};function g(e){let n={a:"a",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",h4:"h4",img:"img",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,s.jsxs)(r.A,{children:[(0,s.jsx)(n.h1,{children:"OpenCV Image Processing Tutorial for Python 3.13"}),(0,s.jsx)("br",{}),(0,s.jsx)(n.h2,{children:"Table of Contents"}),(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"This article has a lot of content ahead. Below is a table of contents for your convenience:"})}),(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"#intro",children:"Introduction to OpenCV and Image Basics"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"#image-math",children:"Image Math: Averages, Blur, and Unsharp"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"#resizing",children:"Resizing Images"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"#channel-alts",children:"Color Channel Alterations"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"#thresholding",children:"Thresholding"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"#denoise",children:"Image Denoising"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"#all-together",children:"Putting It All Together"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"#links",children:"Links and Further Reading"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"#summary",children:"Summary"})}),"\n"]}),(0,s.jsxs)(n.h2,{children:["Introduction to OpenCV and Image Basics ",(0,s.jsx)("a",{id:"intro"})]}),(0,s.jsx)(n.p,{children:'OpenCV (Open Source Computer Vision Library) is a powerful tool for working with images and videos. Think of it as a toolkit that helps\ncomputers "see" and understand images the same way humans do. When we look at a photo, we automatically recognize faces, objects, and colors.\nOpenCV gives us the tools to teach computers to do the same thing pretty easily.'}),(0,s.jsxs)(n.p,{children:["The image we will work with for this tutorial is provided ",(0,s.jsx)(n.a,{href:"/images/pfw_logo+test.png",children:"here"}),"."]}),(0,s.jsx)(n.h3,{children:"What is a Digital Image?"}),(0,s.jsx)(n.p,{children:"A digital image is essentially a grid of tiny colored dots called 'pixels'. Each pixel has a specific color value. For example, a black and\nwhite image might have pixels with values from 0 (completely black) to 255 (completely white), or a pure binary representation of 0 (for completely black) or 1 (for completely white).\nColor images typically use three channels - Red, Green,and Blue (RGB) - with each channel having values from 0 to 255. The way that these intensities combine in the human\neye's perception is what produces the illusion of a distinct color (and why smaller pixels tend to improve quality up to a point)."}),(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{src:"/images/LCD_TFT_Screen_Closeup.jpg",alt:"LCD Image Closeup Wikimedia Commons",title:"LCD Screen"})}),(0,s.jsx)("br",{}),(0,s.jsx)(n.h4,{children:"Why represent images like this?"}),(0,s.jsx)(n.p,{children:"There are alternative ways to represent images such as representing their components using signals, or representing the information they convey & rasterizing an image from that\n(i.e. vector images). These representations have benefits, such as being better for compression, easier to transmit, lossless in quality/scale, or fitting well with other\nsimilar data!"}),(0,s.jsx)(n.p,{children:"However, we choose to use pixels because they are tractable and intuitive. If you represent an image's components as a signal, you heighten the required math and understanding\nneeded to work with the image alongside making altering individual image pieces more difficult. If you represent an image in an alternative format and convert it into pixels\nwhenever you wish to display it, you incur a performance cost."}),(0,s.jsx)(n.p,{children:"Pixels are discrete and defined units of information. You know the range of their values, and often know all the dimensions (width, height, channels), and thus you know the image size and possible\ncombinations. You can speak of individual pixels or individual pixel components entirely separately from their neighboring channels or pixels. Pixels also have a somewhat\nimplicit representation as an array or matrix, which makes performing math on them straightforward with computers (and matrix operations). This is a representation with\ntradeoffs - images using pixels are lossy, they don't rotate well without interpolation, and they don't always implicitly lend themselves to advanced compression insights -\nbut in the real world these are often insignificant relative to the benefits."}),(0,s.jsx)("br",{}),(0,s.jsx)(n.h4,{children:"Why use OpenCV? A story, and an explanation:"}),(0,s.jsx)(n.p,{children:"We use OpenCV when we want to make decisions based on image data."}),(0,s.jsx)(n.p,{children:"Let's imagine a scenario where we own many orange trees because they're pretty, and our friend\nmakes a bet with us - they bet they have more oranges on their trees. Both you and your friend have many many orange trees, and neither of you have the time to count\noranges, but you must win this bet! If only there was a way you could count every orange on every tree reliably - a way that could work on both of your beautiful yards."}),(0,s.jsxs)(n.p,{children:["You sit in a stupor, mind spinning for a solution, until you recall something from an ACM meeting! You can make decisions based on the data stored within images.\nLuckily for you, you have a high resolution camera and both you and your neighbor have photogenic yards. You recall something you can do, known as color detection.\nBy detecting orange spots within an image, you can detect oranges in the trees! This works brilliantly - until it doesn't. You quickly realize that detecting a single\ncolor within the images counts ",(0,s.jsx)(n.em,{children:"some"})," oranges, but it ",(0,s.jsx)(n.em,{children:"only"})," counts ",(0,s.jsx)(n.em,{children:"some"})," oranges. Oranges in the shadow or sun have a different color!"]}),(0,s.jsxs)(n.p,{children:["Luckily you paid attention, and you remember that it's possible to detect colors within a certain ",(0,s.jsx)(n.em,{children:"distance"})," of another color! So you modify your program to detect\nevery shade of orange near your chosen shade, and you manage to detect all of the oranges!! However your neighbor is quick to point out that you have orange stones\nwhile they have blueish stones. You check your output, and surely enough it's counting parts of your house!"]}),(0,s.jsx)(n.p,{children:"You ask a peer for help, and they suggest you check the shape and area of the shapes you detect - and finally you have a solution! You win the bet!"}),(0,s.jsx)(n.p,{children:(0,s.jsx)(n.a,{href:"https://interactables.pfw-acm.org/color-detection",children:"Color detection example."})}),(0,s.jsx)(o.A,{images:[{src:"/images/orange_trees_KristofferTrolle.jpg",comment:"A picture of your orange trees."},{src:"/images/oranges_low_thresh.png",comment:"An image representing detected pixels (white) and ignored pixels (black). Known as Thresholding."},{src:"/images/oranges_low_detection.png",comment:"An image of your first attempt at orange detection"},{src:"/images/oranges_decent_detection.png",comment:"A picture of a later attempt at orange detection."},{src:"/images/oranges_over_detection.png",comment:"An image of you overdetecting just a bit..."},{src:"/images/oranges_over_thresh.png",comment:"A Threshold image with overdetection"}]}),(0,s.jsx)("br",{}),(0,s.jsx)(n.p,{children:"Of course, these skills generalize beyond detecting oranges in hobby projects (though that's a great place to start!).\nImage processing can be used on the production line, in the sciences and medicines for detection or correction, in the arts for cool processing effects and edits,\nin machine learning for recognition or prediction (faces, weather, etc) - really anywhere we can see or make images we can use image processing. OpenCV provides a toolkit\nto do this! So let's get into it."}),(0,s.jsx)(d.XF,{children:(0,s.jsx)(n.p,{children:"Ensure to install OpenCV before attempting to run the code!"})}),(0,s.jsx)("br",{}),(0,s.jsx)(n.p,{children:"Let's start with a simple example that loads and displays an image:"}),(0,s.jsx)(l.zI,{code:"import cv2\n\n# Load the image\nimage = cv2.imread('your_image.jpg')  # Replace with your image file path\n\n# Check if the image was loaded successfully\nif image is None:\n  print(\"Failed to load image.\")\nelse:\n  # Display the image\n  cv2.imshow('Image', image)\n  cv2.waitKey(0)\n  cv2.destroyAllWindows()",tooltips:l.Ss,theme:"dark",showLineNumbers:!0,copyButton:!0,className:"mb-6"}),(0,s.jsx)(n.p,{children:"Let's walk through the program briefly."}),(0,s.jsxs)(n.p,{children:["The image ",(0,s.jsx)(n.code,{children:"your_image.jpg"})," (or whatever path you provide) will be returned in array form by ",(0,s.jsx)(n.code,{children:"cv2.imread"}),". We store this in a variable."]}),(0,s.jsx)(n.p,{children:"We check that this variable isn't undefined (meaning we successfully got an image)."}),(0,s.jsxs)(n.p,{children:["And we produce a popup with the title ",(0,s.jsx)(n.code,{children:"Image"}),", displaying the image stored in the variable ",(0,s.jsx)(n.code,{children:"image"}),"."]}),(0,s.jsx)("br",{}),(0,s.jsx)(n.p,{children:"If we're willing to use Tkinter, we can add some basic UI functionality!"}),(0,s.jsx)(l.zI,{code:"import cv2\nfrom tkinter import Tk, filedialog\n\n# Hide the root Tk window\nTk().withdraw()\n\n# Open file dialog\nfile_path = filedialog.askopenfilename()\n\n# Load and display image\nimage = cv2.imread(file_path)\nif image is not None:\n  cv2.imshow('Selected Image', image)\n  cv2.waitKey(0)\n  cv2.destroyAllWindows()\nelse:\n  print(\"Failed to load image.\")",tooltips:l.Ss,theme:"dark",showLineNumbers:!0,copyButton:!0,className:"mb-6"}),(0,s.jsx)(n.p,{children:"Or for a more advanced example, we can start to work with some of the image properties OpenCV provides:"}),(0,s.jsx)(l.zI,{code:'import cv2\nimport tkinter as tk\nfrom tkinter import filedialog\nimport numpy as np\n\ndef select_and_display_image():\n  # Create a simple file dialog to select an image\n  root = tk.Tk()\n  root.withdraw()  # Hide the main window\n  \n  # Ask user to select an image file\n  file_path = filedialog.askopenfilename(\n      title="Select an Image",\n      filetypes=[("Image files", "*.jpg *.jpeg *.png *.bmp *.tiff")]\n  )\n  \n  if file_path:\n      # Load the image\n      image = cv2.imread(file_path)\n      \n      # Display basic information about the image\n      height, width = image.shape[:2]\n      print(f"Image size: {width} x {height} pixels")\n      print(f"Image shape: {image.shape}")\n      \n      # Display the image\n      cv2.imshow(\'Original Image\', image)\n      cv2.waitKey(0)  # Wait for any key press\n      cv2.destroyAllWindows()\n  \n  root.destroy()\n\n# Run the function\nselect_and_display_image()',tooltips:l.Ss,theme:"dark",showLineNumbers:!0,copyButton:!0,className:"mb-6"}),(0,s.jsx)(n.p,{children:"Output:"}),(0,s.jsx)(h.A,{children:(0,s.jsx)(n.p,{children:"Image size: 206 x 245 pixels\nImage shape: (245, 206, 3)"})}),(0,s.jsx)("br",{}),(0,s.jsxs)(n.p,{children:["Image output:\n",(0,s.jsx)(n.img,{src:"/images/pfw_opencv.png",alt:"image",title:"Image output of the Purdue Fort Wayne Logo"})]}),(0,s.jsxs)(n.p,{children:["This script demonstrates the basic workflow: select a file, load it with OpenCV, and display it. The ",(0,s.jsx)(n.code,{children:"cv2.imread()"})," function loads the image\ninto memory as a numpy array, which is essentially a big grid of numbers representing pixel values."]}),(0,s.jsx)(d.T0,{title:"Numpy Arrays",children:(0,s.jsxs)(n.p,{children:["Numpy Arrays are essentially the same as C-style arrays, but with specific higher level operations provided. In the case of an image, the dimensionality\nwill generally be a H x W x D or - [H][W][D] array - where ",(0,s.jsx)(n.em,{children:"H"})," represents the image height, ",(0,s.jsx)(n.em,{children:"W"})," represents the image width, and ",(0,s.jsx)(n.em,{children:"D"})," represents the channel count\nor depth. So an RGBA (rgb with 'alpha' or transparency, like PNG) image with a height of 200 pixels and a width of 40 pixels would be represented by an array like ",(0,s.jsx)(n.code,{children:"image[200][40][4]"}),"."]})}),(0,s.jsx)("br",{}),(0,s.jsxs)(n.p,{children:["We store this array (Python list) into a variable, and check its first two dimensions - which provides us the Height and Width. We do this using a list comprehension,\nthe ",(0,s.jsx)(n.code,{children:"image.shape[:2]"})," operation. ",(0,s.jsx)(n.code,{children:"image.shape"})," (or the ",(0,s.jsx)(n.code,{children:".shape"})," property of any numpy image object) will return a tuple/list of the form ",(0,s.jsx)(n.code,{children:"(height, width, channels)"}),".\nThe ",(0,s.jsx)(n.code,{children:"[:2]"})," operation reads the first two array values starting from the left."]}),(0,s.jsxs)(n.h2,{children:["Image Math: Averages, Blur, and Unsharp ",(0,s.jsx)("a",{id:"image-math"})]}),(0,s.jsx)(n.h3,{children:"Understanding Image Averaging"}),(0,s.jsx)(n.p,{children:"Image averaging is like taking the average score of a test - you add up all the values and divide by how many there are. In images, we often\naverage pixel values with their neighbors to create different effects."}),(0,s.jsx)(n.p,{children:"When we average neighboring pixels, we get a smoother, less detailed image. This is the foundation of many image processing techniques."}),(0,s.jsx)(n.h3,{children:"Blur: Smoothing Out the Details"}),(0,s.jsx)(n.p,{children:"Blurring is like looking at something through frosted glass - the basic shapes are there, but the fine details are softened. Mathematically,\nwe replace each pixel with the average of itself and its neighbors."}),(0,s.jsx)(l.zI,{code:'import cv2\nimport tkinter as tk\nfrom tkinter import filedialog\nimport numpy as np\n\ndef blur_image_example():\n  # Select image file\n  root = tk.Tk()\n  root.withdraw()\n  \n  file_path = filedialog.askopenfilename(\n      title="Select an Image to Blur",\n      filetypes=[("Image files", "*.jpg *.jpeg *.png *.bmp *.tiff")]\n  )\n  \n  if file_path:\n      # Load the image\n      original = cv2.imread(file_path)\n      \n      # Apply different types of blur\n      # Simple average blur - replaces each pixel with average of surrounding pixels\n      simple_blur = cv2.blur(original, (15, 15))  # 15x15 kernel size\n      \n      # Gaussian blur - gives more weight to closer pixels (more natural looking)\n      gaussian_blur = cv2.GaussianBlur(original, (15, 15), 0)\n      \n      # Display results\n      cv2.imshow(\'Original\', original)\n      cv2.imshow(\'Simple Blur\', simple_blur)\n      cv2.imshow(\'Gaussian Blur\', gaussian_blur)\n      \n      print("Press any key to close windows")\n      cv2.waitKey(0)\n      cv2.destroyAllWindows()\n  \n  root.destroy()\n\nblur_image_example()',tooltips:l.Ss,theme:"dark",showLineNumbers:!0,copyButton:!0,className:"mb-6"}),(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.img,{src:"/images/pfw_simpleblur.png",alt:"image",title:"Simple blur on the Purdue Fort Wayne logo"}),(0,s.jsx)("br",{}),"\n",(0,s.jsx)(n.img,{src:"/images/pfw_gaussianblur.png",alt:"image",title:"Gaussian blur on the Purdue Fort Wayne logo"})]}),(0,s.jsx)(n.h3,{children:"Unsharp Masking: Making Images Crisper"}),(0,s.jsx)(n.p,{children:"Unsharp masking sounds backwards - we use blurring to make images sharper! Here's how it works: we subtract a blurred version from the\noriginal image to find the edges and details, then add those back to the original image to make it appear sharper."}),(0,s.jsx)(l.zI,{code:'import cv2\nimport tkinter as tk\nfrom tkinter import filedialog\nimport numpy as np\n\ndef unsharp_mask_example():\n  root = tk.Tk()\n  root.withdraw()\n  \n  file_path = filedialog.askopenfilename(\n      title="Select an Image to Sharpen",\n      filetypes=[("Image files", "*.jpg *.jpeg *.png *.bmp *.tiff")]\n  )\n  \n  if file_path:\n      original = cv2.imread(file_path)\n      \n      # Step 1: Create a blurred version\n      blurred = cv2.GaussianBlur(original, (9, 9), 10.0)\n      \n      # Step 2: Find the difference (this highlights edges and details)\n      unsharp_mask = cv2.addWeighted(original, 1.5, blurred, -0.5, 0)\n      \n      # Display results\n      cv2.imshow(\'Original\', original)\n      cv2.imshow(\'Blurred Version\', blurred)\n      cv2.imshow(\'Unsharp Masked (Sharpened)\', unsharp_mask)\n      \n      print("Notice how the sharpened image has more defined edges!")\n      cv2.waitKey(0)\n      cv2.destroyAllWindows()\n  \n  root.destroy()\n\nunsharp_mask_example()',tooltips:l.Ss,theme:"dark",showLineNumbers:!0,copyButton:!0,className:"mb-6"}),(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.img,{src:"/images/pfw_sharpenblur.png",alt:"image",title:"Blurred version of the Purdue Fort Wayne logo"}),(0,s.jsx)("br",{}),"\n",(0,s.jsx)(n.img,{src:"/images/pfw_sharpened.png",alt:"image",title:"Sharpened version of the Purdue Fort Wayne logo"})]}),(0,s.jsxs)(n.h2,{children:["Resizing Images ",(0,s.jsx)("a",{id:"resizing"})]}),(0,s.jsx)(n.p,{children:"Resizing images is like stretching or shrinking a rubber sheet. When we make an image larger, we need to create new pixels (interpolation).\nWhen we make it smaller, we need to decide which pixels to keep or combine."}),(0,s.jsx)(n.h3,{children:"Important Considerations"}),(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Aspect Ratio"}),": The width-to-height ratio of your image. If you don't maintain this ratio, your image will look stretched or squished."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Quality Loss"}),": Making an image larger than its original size will always result in some quality loss because we're creating information\nthat wasn't originally there."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Interpolation Methods"}),": Different mathematical methods for creating new pixel values when resizing."]}),"\n"]}),"\n"]}),(0,s.jsx)(l.zI,{code:'import cv2\nimport tkinter as tk\nfrom tkinter import filedialog\nimport numpy as np\n\ndef resize_image_example():\n  root = tk.Tk()\n  root.withdraw()\n  \n  file_path = filedialog.askopenfilename(\n      title="Select an Image to Resize",\n      filetypes=[("Image files", "*.jpg *.jpeg *.png *.bmp *.tiff")]\n  )\n  \n  if file_path:\n      original = cv2.imread(file_path)\n      height, width = original.shape[:2]\n      \n      print(f"Original size: {width} x {height}")\n      \n      # Method 1: Resize by specific dimensions (might change aspect ratio)\n      resized_fixed = cv2.resize(original, (400, 300))\n      \n      # Method 2: Resize by percentage (maintains aspect ratio)\n      scale_percent = 50  # 50% of original size\n      new_width = int(width * scale_percent / 100)\n      new_height = int(height * scale_percent / 100)\n      resized_percent = cv2.resize(original, (new_width, new_height))\n      \n      # Method 3: Resize maintaining aspect ratio to fit within bounds\n      max_width, max_height = 600, 400\n      scale = min(max_width/width, max_height/height)\n      new_width = int(width * scale)\n      new_height = int(height * scale)\n      resized_fitted = cv2.resize(original, (new_width, new_height))\n      \n      # Display results\n      cv2.imshow(\'Original\', original)\n      cv2.imshow(\'Fixed Size (400x300)\', resized_fixed)\n      cv2.imshow(\'50% Scale\', resized_percent)\n      cv2.imshow(\'Fitted to 600x400\', resized_fitted)\n      \n      print("Compare the different resizing methods!")\n      cv2.waitKey(0)\n      cv2.destroyAllWindows()\n  \n  root.destroy()\n\nresize_image_example()',tooltips:l.Ss,theme:"dark",showLineNumbers:!0,copyButton:!0,className:"mb-6"}),(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"For reference, the original image size is"})," ",(0,s.jsx)(n.code,{children:"206 x 245"}),(0,s.jsx)("br",{}),(0,s.jsx)("br",{}),"\n",(0,s.jsx)(n.img,{src:"/images/pfw_fixedsize.png",alt:"image",title:"Purdue Fort Wayne logo resized at a size fixed to 400x300, changing its aspect ratio"}),(0,s.jsx)("br",{}),"\n",(0,s.jsx)(n.img,{src:"/images/pfw_halfsmaller.png",alt:"image",title:"Purdue Fort Wayne logo resized to 50% of its original size"}),(0,s.jsx)("br",{}),"\n",(0,s.jsx)(n.img,{src:"/images/pfw_larger.png",alt:"image",title:"Purdue Fort Wayne logo resized to 600x400, maintaining its aspect ratio"})]}),(0,s.jsxs)(n.h2,{children:["Color Channel Alterations ",(0,s.jsx)("a",{id:"channel-alts"})]}),(0,s.jsx)(n.p,{children:"Color images are made up of three separate channels: Red, Green, and Blue. Each channel is like a separate black and white image that\nrepresents how much of that color is present at each pixel. When combined, these three channels create the full-color image we see."}),(0,s.jsx)(n.h3,{children:"Understanding Color Channels"}),(0,s.jsx)(n.p,{children:"Think of it like having three colored flashlights (red, green, blue) shining on a white wall. Where all three overlap, you get white light.\nWhere only red and green overlap, you get yellow. This is how digital color works!"}),(0,s.jsx)(l.zI,{code:"import cv2\nimport tkinter as tk\nfrom tkinter import filedialog\nimport numpy as np\n\ndef color_channel_example():\n  root = tk.Tk()\n  root.withdraw()\n  \n  file_path = filedialog.askopenfilename(\n      title=\"Select a Color Image\",\n      filetypes=[(\"Image files\", \"*.jpg *.jpeg *.png *.bmp *.tiff\")]\n  )\n  \n  if file_path:\n      # Load image (OpenCV loads as BGR, not RGB!)\n      original = cv2.imread(file_path)\n      \n      # Split into individual color channels\n      blue_channel, green_channel, red_channel = cv2.split(original)\n      \n      # Create colored versions of each channel for better visualization\n      # Make empty arrays for the other channels (filled with zeros)\n      zeros = np.zeros(blue_channel.shape[:2], dtype=\"uint8\")\n      \n      # Create colored channel images\n      blue_colored = cv2.merge([blue_channel, zeros, zeros])\n      green_colored = cv2.merge([zeros, green_channel, zeros])\n      red_colored = cv2.merge([zeros, zeros, red_channel])\n      \n      # Convert to grayscale (different methods)\n      gray_average = cv2.cvtColor(original, cv2.COLOR_BGR2GRAY)\n      gray_weighted = cv2.cvtColor(original, cv2.COLOR_BGR2GRAY)\n      \n      # Display results\n      cv2.imshow('Original', original)\n      cv2.imshow('Blue Channel', blue_colored)\n      cv2.imshow('Green Channel', green_colored)\n      cv2.imshow('Red Channel', red_colored)\n      cv2.imshow('Grayscale', gray_average)\n      \n      print(\"Notice how different objects appear brighter in different color channels!\")\n      cv2.waitKey(0)\n      cv2.destroyAllWindows()\n  \n  root.destroy()\n\ncolor_channel_example()",tooltips:l.Ss,theme:"dark",showLineNumbers:!0,copyButton:!0,className:"mb-6"}),(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.img,{src:"/images/pfw_red.png",alt:"image",title:"Purdue Fort Wayne logo on the red channel"}),(0,s.jsx)("br",{}),"\n",(0,s.jsx)(n.img,{src:"/images/pfw_green.png",alt:"image",title:"Purdue Fort Wayne logo on the green channel"}),(0,s.jsx)("br",{}),"\n",(0,s.jsx)(n.img,{src:"/images/pfw_blue.png",alt:"image",title:"Purdue Fort Wayne logo on the blue channel"}),(0,s.jsx)("br",{}),"\n",(0,s.jsx)(n.img,{src:"/images/pfw_grayscale.png",alt:"image",title:"Purdue Fort Wayne logo as grayscale"})]}),(0,s.jsx)(n.h3,{children:"Converting Between Color Spaces"}),(0,s.jsx)(n.p,{children:"Sometimes it's useful to work in different color spaces. HSV (Hue, Saturation, Value) is often easier for detecting specific colors because it\nseparates color information from brightness."}),(0,s.jsx)(l.zI,{code:"import cv2\nimport tkinter as tk\nfrom tkinter import filedialog\nimport numpy as np\n\ndef color_space_conversion():\n  root = tk.Tk()\n  root.withdraw()\n  \n  file_path = filedialog.askopenfilename(\n      title=\"Select an Image for Color Space Conversion\",\n      filetypes=[(\"Image files\", \"*.jpg *.jpeg *.png *.bmp *.tiff\")]\n  )\n  \n  if file_path:\n      original = cv2.imread(file_path)\n      \n      # Convert to different color spaces\n      hsv_image = cv2.cvtColor(original, cv2.COLOR_BGR2HSV)\n      lab_image = cv2.cvtColor(original, cv2.COLOR_BGR2LAB)\n      \n      # Split HSV channels\n      hue, saturation, value = cv2.split(hsv_image)\n      \n      # Display results\n      cv2.imshow('Original (BGR)', original)\n      cv2.imshow('HSV Version', hsv_image)\n      cv2.imshow('Hue Channel', hue)\n      cv2.imshow('Saturation Channel', saturation)\n      cv2.imshow('Value/Brightness Channel', value)\n      \n      print(\"HSV separates color (hue) from brightness (value)!\")\n      cv2.waitKey(0)\n      cv2.destroyAllWindows()\n  \n  root.destroy()\n\ncolor_space_conversion()",tooltips:l.Ss,theme:"dark",showLineNumbers:!0,copyButton:!0,className:"mb-6"}),(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.img,{src:"/images/pfw_hsv.png",alt:"image",title:"Purdue Fort Wayne logo in the HSV format"}),(0,s.jsx)("br",{}),"\n",(0,s.jsx)(n.img,{src:"/images/pfw_hue.png",alt:"image",title:"Purdue Fort Wayne logo in the hue format"}),(0,s.jsx)("br",{}),"\n",(0,s.jsx)(n.img,{src:"/images/pfw_saturation.png",alt:"image",title:"Purdue Fort Wayne logo in the saturation format"}),(0,s.jsx)("br",{}),"\n",(0,s.jsx)(n.img,{src:"/images/pfw_value_or_brightness.png",alt:"image",title:"Purdue Fort Wayne logo in the value/brightness format"})]}),(0,s.jsxs)(n.h2,{children:["Thresholding ",(0,s.jsx)("a",{id:"thresholding"})]}),(0,s.jsx)(n.p,{children:'Thresholding is like sorting things into two groups: "light enough" and "too dark" (or vice versa). It\'s a simple way to grayscale images\ninto binary (black and white) images.'}),(0,s.jsx)(n.p,{children:'Imagine you\'re looking at a page of text. Your eyes automatically separate the dark text from the light background. Thresholding does the same\nthing - it picks a brightness level and says "anything brighter than this becomes white, anything darker becomes black."'}),(0,s.jsx)(n.h3,{children:"Basic Thresholding"}),(0,s.jsx)(l.zI,{code:"import cv2\nimport tkinter as tk\nfrom tkinter import filedialog\nimport numpy as np\n\ndef basic_thresholding():\n  root = tk.Tk()\n  root.withdraw()\n  \n  file_path = filedialog.askopenfilename(\n      title=\"Select an Image for Thresholding\",\n      filetypes=[(\"Image files\", \"*.jpg *.jpeg *.png *.bmp *.tiff\")]\n  )\n  \n  if file_path:\n      original = cv2.imread(file_path)\n      \n      # Convert to grayscale first (thresholding works on grayscale)\n      gray = cv2.cvtColor(original, cv2.COLOR_BGR2GRAY)\n      \n      # Apply different threshold values\n      # Anything below threshold becomes 0 (black), above becomes 255 (white)\n      _, thresh1 = cv2.threshold(gray, 127, 255, cv2.THRESH_BINARY)\n      _, thresh2 = cv2.threshold(gray, 100, 255, cv2.THRESH_BINARY)\n      _, thresh3 = cv2.threshold(gray, 180, 255, cv2.THRESH_BINARY)\n      \n      # Inverse thresholding (flip black and white)\n      _, thresh_inv = cv2.threshold(gray, 127, 255, cv2.THRESH_BINARY_INV)\n      \n      # Display results\n      cv2.imshow('Original', original)\n      cv2.imshow('Grayscale', gray)\n      cv2.imshow('Threshold = 127', thresh1)\n      cv2.imshow('Threshold = 100 (more white)', thresh2)\n      cv2.imshow('Threshold = 180 (more black)', thresh3)\n      cv2.imshow('Inverse Threshold', thresh_inv)\n      \n      print(\"Try different threshold values to see how they affect the result!\")\n      cv2.waitKey(0)\n      cv2.destroyAllWindows()\n  \n  root.destroy()\n\nbasic_thresholding()",tooltips:l.Ss,theme:"dark",showLineNumbers:!0,copyButton:!0,className:"mb-6"}),(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.img,{src:"/images/pfw_grayscale_threshold.png",alt:"image",title:"Purdue Fort Wayne logo with a grayscale threshold"}),(0,s.jsx)("br",{}),"\n",(0,s.jsx)(n.img,{src:"/images/pfw_127_threshold.png",alt:"image",title:"Purdue Fort Wayne logo with a threshold value of 127"}),(0,s.jsx)("br",{}),"\n",(0,s.jsx)(n.img,{src:"/images/pfw_100_threshold.png",alt:"image",title:"Purdue Fort Wayne logo with a threshold value of 100"}),(0,s.jsx)("br",{}),"\n",(0,s.jsx)(n.img,{src:"/images/pfw_180_threshold.png",alt:"image",title:"Purdue Fort Wayne logo with a threshold value of 180"}),(0,s.jsx)("br",{}),"\n",(0,s.jsx)(n.img,{src:"/images/pfw_inverse_threshold.png",alt:"image",title:"Purdue Fort Wayne logo with an inverse threshold"})]}),(0,s.jsx)(n.h3,{children:"Adaptive Thresholding"}),(0,s.jsx)(n.p,{children:"Sometimes a single threshold value doesn't work well for the entire image. For example, if part of your image is in shadow and part is in\nbright light, you need different thresholds for different areas. Adaptive thresholding automatically adjusts the threshold based on the local\nneighborhood of each pixel."}),(0,s.jsx)(l.zI,{code:"import cv2\nimport tkinter as tk\nfrom tkinter import filedialog\nimport numpy as np\n\ndef adaptive_thresholding():\n  root = tk.Tk()\n  root.withdraw()\n  \n  file_path = filedialog.askopenfilename(\n      title=\"Select an Image for Adaptive Thresholding\",\n      filetypes=[(\"Image files\", \"*.jpg *.jpeg *.png *.bmp *.tiff\")]\n  )\n  \n  if file_path:\n      original = cv2.imread(file_path)\n      gray = cv2.cvtColor(original, cv2.COLOR_BGR2GRAY)\n      \n      # Global thresholding (same threshold for entire image)\n      _, global_thresh = cv2.threshold(gray, 127, 255, cv2.THRESH_BINARY)\n      \n      # Adaptive thresholding - threshold changes based on local area\n      adaptive_mean = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_MEAN_C,\n                                          cv2.THRESH_BINARY, 11, 2)\n      \n      adaptive_gaussian = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n                                              cv2.THRESH_BINARY, 11, 2)\n      \n      # Display results\n      cv2.imshow('Original', original)\n      cv2.imshow('Global Threshold', global_thresh)\n      cv2.imshow('Adaptive Mean', adaptive_mean)\n      cv2.imshow('Adaptive Gaussian', adaptive_gaussian)\n      \n      print(\"Adaptive thresholding works better with uneven lighting!\")\n      cv2.waitKey(0)\n      cv2.destroyAllWindows()\n  \n  root.destroy()\n\nadaptive_thresholding()",tooltips:l.Ss,theme:"dark",showLineNumbers:!0,copyButton:!0,className:"mb-6"}),(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.img,{src:"/images/pfw_global_threshold.png",alt:"image",title:"Purdue Fort Wayne logo with a global threshold"}),(0,s.jsx)("br",{}),"\n",(0,s.jsx)(n.img,{src:"/images/pfw_adaptive_mean.png",alt:"image",title:"Purdue Fort Wayne logo with an adaptive mean threshold"}),(0,s.jsx)("br",{}),"\n",(0,s.jsx)(n.img,{src:"/images/pfw_adaptive_mean.png",alt:"image",title:"Purdue Fort Wayne logo with a adaptive gaussian threshold"})]}),(0,s.jsxs)(n.h2,{children:["Image Denoising ",(0,s.jsx)("a",{id:"denoise"})]}),(0,s.jsx)(n.p,{children:"Image noise is like static on an old TV - random variations in pixel values that don't represent the actual image content. Noise can come from\ncamera sensors, compression, or transmission errors. Denoising is the process of removing this unwanted noise while preserving the important\ndetails of the image."}),(0,s.jsx)(n.h3,{children:"Understanding Noise"}),(0,s.jsx)(n.p,{children:"Digital noise appears as random speckles or grain in your image. It's especially visible in photos taken in low light conditions or with high\nISO settings. The challenge with denoising is removing the noise without also removing important details like edges and textures."}),(0,s.jsx)(l.zI,{code:"import cv2\nimport tkinter as tk\nfrom tkinter import filedialog\nimport numpy as np\n\ndef image_denoising():\n  root = tk.Tk()\n  root.withdraw()\n  \n  file_path = filedialog.askopenfilename(\n      title=\"Select a Noisy Image\",\n      filetypes=[(\"Image files\", \"*.jpg *.jpeg *.png *.bmp *.tiff\")]\n  )\n  \n  if file_path:\n      original = cv2.imread(file_path)\n      \n      # Apply different denoising methods\n      \n      # Method 1: Simple Gaussian blur (removes noise but also blurs details)\n      gaussian_denoised = cv2.GaussianBlur(original, (5, 5), 0)\n      \n      # Method 2: Non-local means denoising (color image)\n      # This method is smarter - it looks for similar patches in the image\n      nlm_denoised = cv2.fastNlMeansDenoisingColored(original, None, 10, 10, 7, 21)\n      \n      # Method 3: Bilateral filter (preserves edges while reducing noise)\n      bilateral_denoised = cv2.bilateralFilter(original, 9, 75, 75)\n      \n      # For grayscale images, we can also try:\n      gray = cv2.cvtColor(original, cv2.COLOR_BGR2GRAY)\n      gray_denoised = cv2.fastNlMeansDenoising(gray, None, 10, 7, 21)\n      \n      # Display results\n      cv2.imshow('Original', original)\n      cv2.imshow('Gaussian Blur', gaussian_denoised)\n      cv2.imshow('Non-Local Means', nlm_denoised)\n      cv2.imshow('Bilateral Filter', bilateral_denoised)\n      cv2.imshow('Grayscale Denoised', gray_denoised)\n      \n      print(\"Compare the different denoising methods!\")\n      print(\"Non-local means preserves details better but takes longer to compute.\")\n      cv2.waitKey(0)\n      cv2.destroyAllWindows()\n  \n  root.destroy()\n\nimage_denoising()",tooltips:l.Ss,theme:"dark",showLineNumbers:!0,copyButton:!0,className:"mb-6"}),(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.img,{src:"/images/don.jpg",alt:"image",title:"Don the Mastodon"}),(0,s.jsx)("br",{}),"\n",(0,s.jsx)(n.img,{src:"/images/don_gaussian_blur.png",alt:"image",title:"Don the Mastodon with gaussian blur"}),(0,s.jsx)("br",{}),"\n",(0,s.jsx)(n.img,{src:"/images/don_non-local_means.png",alt:"image",title:"Don the Mastodon with non-local means"}),"\n",(0,s.jsx)(n.img,{src:"/images/don_bilateral_filter.png",alt:"image",title:"Don the Mastodon with the bilateral filter"}),(0,s.jsx)("br",{}),"\n",(0,s.jsx)(n.img,{src:"/images/don_grayscale_denoised.png",alt:"image",title:"Don the Mastodon denoised with grayscale"}),(0,s.jsx)("br",{})]}),(0,s.jsx)(n.h3,{children:"Adding Noise for Testing"}),(0,s.jsx)(n.p,{children:"Sometimes it's helpful to add controlled noise to an image so you can test your denoising algorithms:"}),(0,s.jsx)(l.zI,{code:"import cv2\nimport tkinter as tk\nfrom tkinter import filedialog\nimport numpy as np\n\ndef add_noise_and_denoise():\n  root = tk.Tk()\n  root.withdraw()\n  \n  file_path = filedialog.askopenfilename(\n      title=\"Select a Clean Image to Add Noise\",\n      filetypes=[(\"Image files\", \"*.jpg *.jpeg *.png *.bmp *.tiff\")]\n  )\n  \n  if file_path:\n      original = cv2.imread(file_path)\n      \n      # Add Gaussian noise\n      noise = np.random.normal(0, 25, original.shape).astype(np.uint8)\n      noisy_image = cv2.add(original, noise)\n      \n      # Add salt and pepper noise\n      salt_pepper = original.copy()\n      # Add salt (white) noise\n      salt_coords = np.random.randint(0, original.shape[0], int(0.005 * original.size))\n      pepper_coords = np.random.randint(0, original.shape[1], int(0.005 * original.size))\n      salt_pepper[salt_coords[0:len(salt_coords)//2], pepper_coords[0:len(pepper_coords)//2]] = 255\n      salt_pepper[salt_coords[len(salt_coords)//2:], pepper_coords[len(pepper_coords)//2:]] = 0\n      \n      # Denoise the noisy images\n      denoised_gaussian = cv2.fastNlMeansDenoisingColored(noisy_image, None, 10, 10, 7, 21)\n      denoised_salt_pepper = cv2.medianBlur(salt_pepper, 5)  # Median filter works well for salt & pepper\n      \n      # Display results\n      cv2.imshow('Original Clean', original)\n      cv2.imshow('With Gaussian Noise', noisy_image)\n      cv2.imshow('Denoised Gaussian', denoised_gaussian)\n      cv2.imshow('With Salt & Pepper Noise', salt_pepper)\n      cv2.imshow('Denoised Salt & Pepper', denoised_salt_pepper)\n      \n      print(\"Different types of noise require different denoising approaches!\")\n      cv2.waitKey(0)\n      cv2.destroyAllWindows()\n  \n  root.destroy()\n\nadd_noise_and_denoise()",tooltips:l.Ss,theme:"dark",showLineNumbers:!0,copyButton:!0,className:"mb-6"}),(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.img,{src:"/images/pfw_gaussian_noise.png",alt:"image",title:"Purdue Fort Wayne logo with gaussian noise"}),(0,s.jsx)("br",{}),"\n",(0,s.jsx)(n.img,{src:"/images/pfw_denoised_gaussian.png",alt:"image",title:"Purdue Fort Wayne logo with gaussian denoising"}),(0,s.jsx)("br",{}),"\n",(0,s.jsx)(n.img,{src:"/images/pfw_salt_and_pepper_noise.png",alt:"image",title:"Purdue Fort Wayne logo with salt and pepper noising"}),(0,s.jsx)("br",{}),"\n",(0,s.jsx)(n.img,{src:"/images/pfw_denoised_salt_and_pepper.png",alt:"image",title:"Purdue Fort Wayne logo with salt and pepper denoising"})]}),(0,s.jsxs)(n.h2,{children:["Putting It All Together ",(0,s.jsx)("a",{id:"all-together"})]}),(0,s.jsx)(n.p,{children:"Now that you've learned the basics of image processing, here's a comprehensive example that lets you apply multiple techniques to any image:"}),(0,s.jsx)(l.zI,{code:'import cv2\nimport tkinter as tk\nfrom tkinter import filedialog, messagebox\nimport numpy as np\n\ndef image_processing_toolkit():\n  root = tk.Tk()\n  root.withdraw()\n  \n  file_path = filedialog.askopenfilename(\n      title="Select an Image for Processing",\n      filetypes=[("Image files", "*.jpg *.jpeg *.png *.bmp *.tiff")]\n  )\n  \n  if not file_path:\n      return\n  \n  original = cv2.imread(file_path)\n  if original is None:\n      messagebox.showerror("Error", "Could not load the image!")\n      return\n  \n  # Create a copy to work with\n  processed = original.copy()\n  \n  while True:\n      # Display current image\n      cv2.imshow(\'Image Processing Toolkit\', processed)\n      cv2.imshow(\'Original (for comparison)\', original)\n      \n      print()\n      print("=== Image Processing Toolkit ===")\n      print("Current image shape:", processed.shape)\n      print("Choose an operation:")\n      print("1. Reset to original")\n      print("2. Convert to grayscale")\n      print("3. Apply Gaussian blur")\n      print("4. Apply unsharp mask (sharpen)")\n      print("5. Apply threshold")\n      print("6. Apply denoising")\n      print("7. Resize image")\n      print("8. Save current image")\n      print("9. Exit (or press ESC)")\n      \n      choice = input("Enter your choice (1-9): ")\n      \n      if choice == \'1\':\n          processed = original.copy()\n          print("Reset to original image.")\n          \n      elif choice == \'2\':\n          processed = cv2.cvtColor(processed, cv2.COLOR_BGR2GRAY)\n          processed = cv2.cvtColor(processed, cv2.COLOR_GRAY2BGR)  # Convert back to 3-channel for consistency\n          print("Converted to grayscale.")\n          \n      elif choice == \'3\':\n          kernel_size = int(input("Enter blur kernel size (odd number, e.g., 15): "))\n          if kernel_size % 2 == 0:\n              kernel_size += 1  # Make sure it\'s odd\n          processed = cv2.GaussianBlur(processed, (kernel_size, kernel_size), 0)\n          print(f"Applied Gaussian blur with kernel size {kernel_size}.")\n          \n      elif choice == \'4\':\n          blurred = cv2.GaussianBlur(processed, (9, 9), 10.0)\n          processed = cv2.addWeighted(processed, 1.5, blurred, -0.5, 0)\n          print("Applied unsharp mask (sharpening).")\n          \n      elif choice == \'5\':\n          gray = cv2.cvtColor(processed, cv2.COLOR_BGR2GRAY)\n          threshold_value = int(input("Enter threshold value (0-255): "))\n          _, thresh = cv2.threshold(gray, threshold_value, 255, cv2.THRESH_BINARY)\n          processed = cv2.cvtColor(thresh, cv2.COLOR_GRAY2BGR)\n          print(f"Applied threshold with value {threshold_value}.")\n          \n      elif choice == \'6\':\n          processed = cv2.fastNlMeansDenoisingColored(processed, None, 10, 10, 7, 21)\n          print("Applied denoising filter.")\n          \n      elif choice == \'7\':\n          scale = float(input("Enter scale factor (e.g., 0.5 for half size, 2.0 for double): "))\n          height, width = processed.shape[:2]\n          new_width = int(width * scale)\n          new_height = int(height * scale)\n          processed = cv2.resize(processed, (new_width, new_height))\n          print(f"Resized image to {new_width}x{new_height}.")\n          \n      elif choice == \'8\':\n          save_path = filedialog.asksaveasfilename(\n              defaultextension=".jpg",\n              filetypes=[("JPEG files", "*.jpg"), ("PNG files", "*.png"), ("All files", "*.*")]\n          )\n          if save_path:\n              cv2.imwrite(save_path, processed)\n              print(f"Image saved to {save_path}")\n          \n      elif choice == \'9\':\n          break\n          \n      else:\n          print("Invalid choice. Please try again.")\n      \n      # Check for ESC key\n      if cv2.waitKey(1) & 0xFF == 27:  # ESC key\n          break\n  \n  cv2.destroyAllWindows()\n  root.destroy()\n\n# Run the toolkit\nimage_processing_toolkit()',tooltips:l.Ss,theme:"dark",showLineNumbers:!0,copyButton:!0,className:"mb-6"}),(0,s.jsxs)(n.h2,{children:["Links and Further Reading ",(0,s.jsx)("a",{id:"links"})]}),(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://docs.opencv.org/4.x/d7/d4d/tutorial_py_thresholding.html",children:(0,s.jsx)(n.strong,{children:"Thresholding"})})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://docs.opencv.org/4.x/db/d8e/tutorial_threshold.html",children:(0,s.jsx)(n.strong,{children:"Advanced Thresholding"})})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://docs.opencv.org/4.x/d5/d69/tutorial_py_non_local_means.html",children:(0,s.jsx)(n.strong,{children:"Denoising"})})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://docs.opencv.org/4.x/d6/d00/tutorial_py_root.html",children:(0,s.jsx)(n.strong,{children:"OpenCV Python Tutorials"})})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://numpy.org/doc/stable/",children:(0,s.jsx)(n.strong,{children:"NumPy Documentation"})})}),"\n"]}),(0,s.jsxs)(n.h2,{children:["Summary ",(0,s.jsx)("a",{id:"summary"})]}),(0,s.jsx)(n.p,{children:"These are a few of the things you should feel semi comfortable with:"}),(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Images are just arrays of numbers"})," - understanding this is key to everything else"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Blurring smooths details"})," - useful for noise reduction and creating artistic effects"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Sharpening enhances edges"})," - the unsharp mask technique is counterintuitive but powerful"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Resizing requires careful consideration"})," - maintain aspect ratios and understand quality trade-offs"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Color channels can be manipulated separately"})," - this opens up many creative possibilities"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Thresholding separates regions"})," - essential for many computer vision tasks"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Denoising removes unwanted artifacts"})," - different methods work better for different types of noise"]}),"\n"]}),(0,s.jsx)("br",{}),(0,s.jsx)(n.p,{children:"These techniques form the foundation for more advanced image processing and computer vision applications. Practice with different images and\nparameter values to build your intuition for how these operations affect different types of images."}),(0,s.jsx)("br",{}),(0,s.jsx)(n.p,{children:"Image"}),(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"PFW Logo Images are property of the ACM, taken by Treasure Chandler"}),"\n",(0,s.jsx)(n.li,{children:"LCD Screen - Property of Wikimedia Commons"}),"\n",(0,s.jsxs)(n.li,{children:["Orange Tree - property of Kristoffer Trolle, CC license ",(0,s.jsx)(n.a,{href:"https://openverse.org/image/dcf7d618-37bb-47f6-a7ba-56404311542d?q=orange+tree&p=1",children:"OrangeTree"})]}),"\n"]})]})}function p(){let e=arguments.length>0&&void 0!==arguments[0]?arguments[0]:{};return(0,s.jsx)(c,{...e,children:(0,s.jsx)(g,{...e})})}},9254:(e,n,i)=>{"use strict";i.d(n,{A:()=>t});var s=i(7876);function t(e){let{title:n="Terminal",children:i,className:t=""}=e;return(0,s.jsxs)("div",{className:"rounded-md border border-[#2e2e2e] bg-[#1e1e1e] text-[#d4d4d4] font-mono mb-3 ".concat(t),children:[(0,s.jsx)("div",{className:"flex justify-between items-center px-4 py-2.5 border-b border-[#2e2e2e] bg-[#2b2b2b]",children:(0,s.jsx)("span",{className:"text-sm text-[#dcdcdc]",children:n})}),(0,s.jsx)("pre",{className:"px-4 py-3 pb-1 pl-10 overflow-x-auto text-sm leading-relaxed whitespace-pre-wrap m-0",children:(0,s.jsx)("code",{className:"text-[#f0f0f0]",children:"string"==typeof i?i.trimEnd().split("\n").map((e,n)=>e.includes("Traceback")?(0,s.jsx)("div",{className:"text-yellow-400",children:e},n):e.includes("NameError")?(0,s.jsx)("div",{className:"text-red-500 font-semibold",children:e},n):(0,s.jsx)("div",{children:e},n)):i})})]})}}},e=>{var n=n=>e(e.s=n);e.O(0,[809,217,636,593,792],()=>n(76)),_N_E=e.O()}]);